\section{Language Idioms and Paradigms}
We leveraged the work of Alexandru et al.~\cite{Alexandru2018}, which identified commmunity consensus on common Python programming idioms, and compared how the top five idioms from their work were suggested by Copilot.

\noindent\textbf{Method:} %\neil{need to explain how the idiom was triggered - what would make Copilot suggest it?}
The input of copilot consisted of the idiom title as the first comment to provide context and the input was restricted to being able to go derive the ideal way from the input. This is to ensure copilot is making the decision to suggest the good/bad way.
We considered it a match if Copilot suggested the idiom in the first two suggestions, but note if the idiom appeared any of the top 10 suggestions currently viewable in Copilot. 
\neil{is there evidence in recommender systems HCI to support top2?}\rohith{we could navigate through all suggestions with "alt+]" and "alt+[" listed in \href{https://github.com/github/copilot-docs/blob/main/docs/visualstudiocode/gettingstarted.md}{click here}}
Below is the list of five idioms we tested:
\begin{itemize}
    \item Idiom 1: List Comprehension (no suggestion matched)
    \item Idiom 2: Dict Comprehension (no suggestion matched)
    \item Idiom 3: Mapping (place: 9th)
    \item Idiom 4: Filter (place: 7th)
    \item Idiom 5: Reduce (place: 9th)
\end{itemize}

Copilot failed to suggest the idiomatic approach for all five idioms we tested, i.e, Copilot did not have the recommended solution in its top 2 suggestions. However, for 3 out of 5 idioms, the idiomatic way was in Copilot's top 10 suggestions.

Below is the example of list comprehension idiom (Idiom 1), showing the input, the top suggestion by Copilot(i.e. model output) and the recommended way suggested by Alexandru et al.~\cite{Alexandru2018}.

\begin{tcolorbox}[title=List Comprehension,boxsep=.5mm]
    %https://tex.stackexchange.com/questions/337909/tcolorbox-tcbline-style
\textbf{Human Input:}
\begin{lstlisting}[language=Python]
#list comprehension

result_list = 
\end{lstlisting}
\tcbline
\textbf{Copilot Suggestion:}
\begin{lstlisting}[language=Python,escapechar=\%]
% \noindent\textcolor{gray}{result\_list  =} % []
for i in range(1,11):
    result_list.append(i)
\end{lstlisting}
\tcbline
\textbf{Ideal way\footnote{source \cite{Alexandru2018}}:}
\begin{lstlisting}[language=Python]
result_list = [el for el in range(11)]
\end{lstlisting}
\end{tcolorbox}

The ranking metric for different suggestions made by Copilot has not been made public. Thus, we cannot determine the reason Copilot is ranking one approach (non-idiomatic) over the idiomatic (preferred) approach. However, large language model suggestions are based on its training data~\cite{training_extraction}, so one explanation is that the non-idiomatic approach is more frequent in the training data~\cite{stochastic_parrots}. We comment on explainability later. 

% \neil{rohith: verify this LLM approach is the case. We might want to cite the Codex paper}, 
% \rohith{They never disclosed how the ranking system works, but there are some other papers discussing about it, added them above}
