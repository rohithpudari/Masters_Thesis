\section{Threats to Validity}
\label{limitations}
Copilot and its underlying OpenAI Codex LLM are not open sources. 
%Accessing them is restricted to API calls, and General users cannot directly examine the model. 
We base our conclusions on API results, which complicate versioning and clarity. There are several threats to the validity of the work presented in this thesis. In this section, we summarize the dangers and also present the steps taken to mitigate them. We use the stable Copilot extension release (version: 1.30.6165) in Visual Studio Code. %The manual effort needed to query Copilot restricts the ability to use large datasets.

\subsection{Internal Validity}
Copilot is sensitive to user inputs, which hurts replicability as a different formulation of the problem might produce a different set of suggestions. 
Thus a reasonable concern is that our (human) input is unfair to Copilot, and with some different inputs, the tool might generate the correct idiom. 
For replicability, we archived all examples in our replication package at \repl{}.
%\neil{can we try this? }\rohith{idioms are mostly one-lined, and longer inputs/suggestions could be described as bad practices/design smells}

\subsection{Construct Validity}
The taxonomy of the software abstraction hierarchy presented in this thesis relies on our view of software abstractions.
Other approaches for classifying software abstractions~(such as the user's motivation for initiating \cct{}) might result in different taxonomy.
The hierarchy of software abstractions presented in this thesis relies on our understanding of software abstractions, and the results of Copilot code suggestions on language idioms and code smells. Further, we present our results using Python and JavaScript. It is possible that using some other programming language or \cct{} might have different results.

\subsection{Bias, Fairness, and representation}
Code generation models are susceptible to repeating the flaws and biases of their training data, just like natural language models~\cite{Gpt3}. These models can reinforce and maintain societal stereotypes when trained on a variety of corpora of human data, having a disproportionately negative effect on underprivileged communities. 
Additionally, bias may result in outdated APIs or low-quality code that reproduces problems, compromising performance and security. This might result in fewer people using new programming languages or libraries.

We intended to show where Copilot cannot consistently generate the preferred answer. We biased our evaluation to highlight this by choosing input that simulates what a less experienced programmer might enter. 
But we argue this is reasonable: for one, these are precisely the developers likely to use Copilot suggestions and unlikely to know the idiomatic usage.
More importantly, a lack of suggestion stability seems to come with its own set of challenges, which are equally important to understand.% (to use an analogy, we would expect all self-driving vehicles to make similar choices when confronted with similar circumstances). 
%this gets to the top levels of Koopmann's pyramid.
% Access to Copilot is currently restricted. 
% Working with GitHub's API - not open
%
%OpenAI and open source

\subsection{Ethical Considerations}
\label{ethics}
Codex imports packages at different rates~\cite{copilot}, which could advantage some authors over others. As Copilot is both a \emph{black-box} and \emph{closed-source}, One can only speculate on the reasoning behind each suggestion. For example, what sort algorithm to use. Not just a technical question but much more ethically challenging on why a particular algorithm is suggested than a flash fill such as a coding contest question.

Additionally, malicious individuals might also utilize \cct{}. Better tools might make it simpler to generate new malware variants, enabling them to evade detection by security software~(which often relies on databases of file fingerprints). 
Tools that increase developer productivity would likewise increase developer productivity when creating harmful malware.