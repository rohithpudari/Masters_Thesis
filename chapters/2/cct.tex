\section{Evolution of \cct{}}
A variety of data science and machine learning methodologies were applied in the field of \cct{}. The following is an overview of current state-of-the-art methodologies for both text and code completion, as well as how these approaches have evolved over time.

\subsection{Statistical language Models}
Statistical language models (LMs) are mathematical models that express a probability distribution across a set of words. From a training dataset, LMs learn the probability of word occurrences. LMs can predict the next word from a sequence of words based on the probability distribution of its training data. Most common LMs like N-grams are currently utilised to support many complicated models in modern approaches like Neural Language Models (NLMs).

\subsubsection{N-grams}
N-grams~\cite{ngram} are probabilistic language models represented by an (n-1)-order Markov source (a sequence of n-1 of random variables defined by a Markov chain). A Markov chain is based on the idea that the probability of the next word depends only on its n-1 predecessors. Since this property mostly holds for natural language, Markov source is a good approach for modeling natural language. In modern natural language processing applications, 
code completion tools tools like SLANG~\cite{slang} and CACHECA~\cite{cacheca} are based on N-grams.

\subsubsection{SLANG}
SLANG~\cite{slang} is a tool for completing gaps~(missing API calls) in an API call sequence. It combines RNNME~\cite{rnnme} with N-grams~(n=3, trigrams). 
The evaluation of the study reveals that using the average of the probabilities generated by the two models for filling in gaps in API call sequences performs better than using either model alone, highlighting the applicability of N-gram LMs in support of more sophisticated models~(RNNME in this case).

\subsubsection{CACHECA}
CACHECA~\cite{cacheca} is an Eclipse IDE~\cite{eclipse} plugin that tries to improve the code completion suggestions provided by Eclipse's default tool. It is created using a Cache language model~\cite{localse}, which provides words that have already been in the text a higher likelihood of occurrence. The localization of code, which asserts that code has local regularities~(a variable will generally occur multiple times in a block of code)~\cite{localse}.

\subsubsection{Problems with N-grams}
N-grams are rarely employed as a stand-alone solution in modern natural language processing applications, but rather as a support for a more complex model, which leads us to common problems with N-gram language models that make them not suitable for effective code completion tools.

\emph{Short distance dependencies -} N-grams can only represent relationships between a word and its n-1 predecessors, which implies they don't have long-distance dependencies. Because it would be difficult to foresee how an imported library function or a initialized variable would be used, this restricts the potential use of N-grams for advanced code completion tasks.

\emph{Out of Vocabulary~(OOV) problem - } In the specific application of code completion, it's possible that a developer utilises a variable or library in our particular application that isn't included in the training set. 
As a result, N-gram models would never predict that term, preventing us from using it as a prediction. This makes the code completion tools restricted to the knowledge of training set, making it impossible to generalise.

% Vector word representations are introduced to tackle the drawbacks of N-grams.
% \subsubsection{Vector Word Representations}
% Each word in the vocabulary is represented as a real-valued feature vector in a vector word representation. Words that are semantically similar in such forms are similar. Vector word representations play an important role in Neural language models~\cite{mnlm}.