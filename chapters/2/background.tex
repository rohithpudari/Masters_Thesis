\startchapter{Background and Related Work}
\label{chapter:background}

\newlength{\savedunitlength}
\setlength{\unitlength}{2em}



\setlength{\unitlength}{\savedunitlength}

\section{Current Challenges}
Some of these basic programming challenges have been already documented and are, we suspect, very much under consideration by the corporate teams behind Copilot and Codex and the like. 

We have been able to show that Copilot can make simple errors, such as not allowing for an empty array in a sort routine\footnote{this and other examples are documented in our replication package.}. Copilot does not understand security vulnerabilities, so it will suggest code that allows for \textsf{log4shell} vulnerability\footnote{\url{https://www.wiz.io/blog/10-days-later-enterprises-halfway-through-patching-log4shell/}}, or common SQL injection attacks. A recent study by Pearce et.al \cite{copilot_security} showed that approximately 40\% of the code suggested by Copilot is found to be vulnerable, when tested on 89 different scenarios for Copilot to complete.
Similarly, concerns have been raised about Copilot licence compliance and copyright violation; with sufficiently tailored input data, Copilot suggests identical code to code on Github under copyright. 

A recent study by karampatsis et al \cite{github_bugs} on 1000 most popular open-source java repositories on GitHub, showed a frequency of one single statement bug per 1600-2500 lines of code and about 33\% of all the bugs match a set of 16 bug templates. This shows that there are bugs which occur repeatedly in the public repositories of GitHub, which can make copilot biased to suggest bug prone code over bug-free version.

But these flaws are not surprising, and seem to present straightforward fixes.
As it is trained on public data collected on a certain date, any code uploaded after that is absent in the knowledge base of Copilot. Copilot, powered by Codex, used training data collected in May 2020 from 50 million public repositories on GitHub\cite{copilot}. So, it does not have any data from useful sources like documentation and Stack Overflow to improve its suggestions from commonly occurring bugs.
Similarly, any flaws present in large numbers on GitHub will tend to dominate the learning of the model (sadly, there is plenty of bad human written code extant!).

Fixes might include better data engineering and filtering, to remove known problems. This is already part of the training of these large language models, although the exact process is not disclosed. 
We might envision security scans or linting runs prior to a suggestion, in order to remove obvious problems like SQL injection. 
Clone detection techniques can help find places where code violates copyright. 
Better machine learning approaches, using active learning or fine-tuning, might help learn local lessons \cite{tim} for customization in the case of identifier naming or formatting.
In most of these cases, good tools exist already for this. 
Thus our belief is that while heuristic, such flaws can be fixed and will be fixed in short order. What we believe is harder to fix will be problems where straight forward corrections may not exist.
