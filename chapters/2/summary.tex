\section{Chapter Summary}
In this chapter, we first provided some background on different language models used for \cct{} and their limitations. We established the importance of transformers for \cct{} which is a crucial component of OpenAI's Codex Model~\cite{copilot}. To further explore the role of transformer architecture in \cct{}, we reviewed studies showing context-sensitive contextualised word representations were presented by LMs such BERT~\cite{bert}.
We then discussed GitHub Copilot, the \cct{} we would use as the basis for our study in this thesis.

Furthermore, we discussed some of the related works on Copilot about its usage~\cite{Vaithilingam2022} and its effectiveness in solving programming contest style problems~\cite{empirical_eval}. We then introduced some of the alternatives that exist for Copilot.
We concluded by discussing the studies that found challenges faced by using \cct{} tools like Copilot and showed that these challenges have straight forward fixes and will be fixed in short order. 

In the next chapters, we discuss the problems with using \cct{} like Copilot that are harder to fix and straight-forward corrections may not exist like language idioms (Chapter~\ref{chapter:idioms}) and code smells (Chapter~\ref{chapter:smells}). 
We then use introduce a taxonomy of software abstraction hierarchy to help with finding the current boundaries of \cct{} like Copilot (Chapter~\ref{chapter:framework}). 
Finally, we discuss some of the practical implications, limitations of our findings and also provide some future directions to help further research in \cct{} (Chapter~\ref{chapter:discussion}).